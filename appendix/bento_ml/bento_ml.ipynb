{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee16cf32",
   "metadata": {},
   "source": [
    "# BentoML - Production ready machine learning\n",
    "\n",
    "* BentoML: open source model serving library\n",
    "* Module 6: developed a model for credit approval\n",
    "    * What needs to be done next?\n",
    "    * How can the model be used by people?\n",
    "    * This can be done as a webservice\n",
    "        * Module 5: wrap into flask app\n",
    "        * This works well in development, but in real-world scenarios more factors need to be considered (especially much more peaople are going to use it)\n",
    "    * Goal of this module: \n",
    "        * build and deploy ML model at scale\n",
    "        * Customize your service to fit your use case\n",
    "        * Make your service production ready\n",
    "* What is 'Production ready'?\n",
    "    * Scalability\n",
    "    * Operationally efficiency\n",
    "    * Repeatability (CI/CD)\n",
    "    * Flexibility\n",
    "    * Resiliency\n",
    "    * Easy to use- ity\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536d0c4f",
   "metadata": {},
   "source": [
    "\"BentoML makes it easy to **create** and **package** your ML service for production\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c485806",
   "metadata": {},
   "source": [
    "# 7.2 Building a Prediction Service\n",
    "* Use model from module 6 from  2022 (copied from github)\n",
    "* Using BentoML we can save the model as it is recommended for each framework and version\n",
    "* See end of notebook: module6_2022.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc92d2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bentoml\n",
      "  Downloading bentoml-1.0.7-py3-none-any.whl (858 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m858.3/858.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting python-multipart\n",
      "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting opentelemetry-semantic-conventions==0.33b0\n",
      "  Downloading opentelemetry_semantic_conventions-0.33b0-py3-none-any.whl (26 kB)\n",
      "Collecting circus\n",
      "  Downloading circus-0.17.1-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.7/182.7 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cattrs>=22.1.0\n",
      "  Downloading cattrs-22.2.0-py3-none-any.whl (35 kB)\n",
      "Collecting opentelemetry-util-http==0.33b0\n",
      "  Downloading opentelemetry_util_http-0.33b0-py3-none-any.whl (6.6 kB)\n",
      "Collecting deepmerge\n",
      "  Downloading deepmerge-1.0.1-py3-none-any.whl (8.0 kB)\n",
      "Collecting starlette\n",
      "  Downloading starlette-0.21.0-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opentelemetry-sdk>=1.9.0\n",
      "  Downloading opentelemetry_sdk-1.13.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.0 in /home/frauke/anaconda3/lib/python3.8/site-packages (from bentoml) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/frauke/anaconda3/lib/python3.8/site-packages (from bentoml) (21.3)\n",
      "Requirement already satisfied: attrs>=21.1.0 in /home/frauke/.local/lib/python3.8/site-packages (from bentoml) (21.4.0)\n",
      "Collecting pynvml<12\n",
      "  Using cached pynvml-11.4.1-py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: Jinja2>=3.0.1 in /home/frauke/.local/lib/python3.8/site-packages (from bentoml) (3.1.2)\n",
      "Collecting uvicorn\n",
      "  Using cached uvicorn-0.18.3-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: cloudpickle in /home/frauke/anaconda3/lib/python3.8/site-packages (from bentoml) (2.0.0)\n",
      "Requirement already satisfied: schema in /home/frauke/anaconda3/lib/python3.8/site-packages (from bentoml) (0.7.2)\n",
      "Collecting fs\n",
      "  Downloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opentelemetry-instrumentation-aiohttp-client==0.33b0\n",
      "  Downloading opentelemetry_instrumentation_aiohttp_client-0.33b0-py3-none-any.whl (11 kB)\n",
      "Collecting pathspec\n",
      "  Downloading pathspec-0.10.1-py3-none-any.whl (27 kB)\n",
      "Collecting python-dotenv>=0.20.0\n",
      "  Using cached python_dotenv-0.21.0-py3-none-any.whl (18 kB)\n",
      "Collecting rich>=11.2.0\n",
      "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/frauke/anaconda3/lib/python3.8/site-packages (from bentoml) (1.21.2)\n",
      "Requirement already satisfied: prometheus-client>=0.10.0 in /home/frauke/.local/lib/python3.8/site-packages (from bentoml) (0.14.1)\n",
      "Collecting pip-tools>=6.6.2\n",
      "  Downloading pip_tools-6.9.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.4/52.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting watchfiles>=0.15.0\n",
      "  Downloading watchfiles-0.17.0-cp37-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /home/frauke/.local/lib/python3.8/site-packages (from bentoml) (2.8.2)\n",
      "Requirement already satisfied: click>=7.0 in /home/frauke/.local/lib/python3.8/site-packages (from bentoml) (7.1.2)\n",
      "Collecting opentelemetry-api>=1.9.0\n",
      "  Downloading opentelemetry_api-1.13.0-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /home/frauke/.local/lib/python3.8/site-packages (from bentoml) (3.8.1)\n",
      "Requirement already satisfied: psutil in /home/frauke/.local/lib/python3.8/site-packages (from bentoml) (5.9.1)\n",
      "Collecting simple-di>=0.1.4\n",
      "  Downloading simple_di-0.1.5-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: requests in /home/frauke/anaconda3/lib/python3.8/site-packages (from bentoml) (2.27.1)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.33b0\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.33b0-py3-none-any.whl (8.9 kB)\n",
      "Collecting opentelemetry-instrumentation==0.33b0\n",
      "  Downloading opentelemetry_instrumentation-0.33b0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in /home/frauke/anaconda3/lib/python3.8/site-packages (from opentelemetry-instrumentation==0.33b0->bentoml) (58.0.4)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /home/frauke/.local/lib/python3.8/site-packages (from opentelemetry-instrumentation==0.33b0->bentoml) (1.14.1)\n",
      "Collecting asgiref~=3.0\n",
      "  Using cached asgiref-3.5.2-py3-none-any.whl (22 kB)\n",
      "Collecting exceptiongroup\n",
      "  Downloading exceptiongroup-1.0.0rc9-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/frauke/.local/lib/python3.8/site-packages (from Jinja2>=3.0.1->bentoml) (2.1.1)\n",
      "Collecting deprecated>=1.2.6\n",
      "  Using cached Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/frauke/.local/lib/python3.8/site-packages (from opentelemetry-sdk>=1.9.0->bentoml) (4.2.0)\n",
      "Collecting opentelemetry-sdk>=1.9.0\n",
      "  Downloading opentelemetry_sdk-1.12.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opentelemetry-api>=1.9.0\n",
      "  Downloading opentelemetry_api-1.12.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/frauke/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->bentoml) (3.0.4)\n",
      "Requirement already satisfied: pip>=21.2 in /home/frauke/anaconda3/lib/python3.8/site-packages (from pip-tools>=6.6.2->bentoml) (22.1.2)\n",
      "Requirement already satisfied: wheel in /home/frauke/anaconda3/lib/python3.8/site-packages (from pip-tools>=6.6.2->bentoml) (0.37.1)\n",
      "Collecting build\n",
      "  Downloading build-0.8.0-py3-none-any.whl (17 kB)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /home/frauke/.local/lib/python3.8/site-packages (from rich>=11.2.0->bentoml) (2.12.0)\n",
      "Requirement already satisfied: anyio<4,>=3.0.0 in /home/frauke/anaconda3/lib/python3.8/site-packages (from watchfiles>=0.15.0->bentoml) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/frauke/.local/lib/python3.8/site-packages (from aiohttp->bentoml) (2.0.12)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/frauke/.local/lib/python3.8/site-packages (from aiohttp->bentoml) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/frauke/.local/lib/python3.8/site-packages (from aiohttp->bentoml) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/frauke/.local/lib/python3.8/site-packages (from aiohttp->bentoml) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/frauke/.local/lib/python3.8/site-packages (from aiohttp->bentoml) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/frauke/.local/lib/python3.8/site-packages (from aiohttp->bentoml) (1.7.2)\n",
      "Requirement already satisfied: tornado>=5.0.2 in /home/frauke/.local/lib/python3.8/site-packages (from circus->bentoml) (6.2)\n",
      "Requirement already satisfied: pyzmq>=17.0 in /home/frauke/.local/lib/python3.8/site-packages (from circus->bentoml) (23.2.0)\n",
      "Requirement already satisfied: six~=1.10 in /home/frauke/anaconda3/lib/python3.8/site-packages (from fs->bentoml) (1.16.0)\n",
      "Requirement already satisfied: appdirs~=1.4.3 in /home/frauke/anaconda3/lib/python3.8/site-packages (from fs->bentoml) (1.4.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/frauke/anaconda3/lib/python3.8/site-packages (from requests->bentoml) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/frauke/anaconda3/lib/python3.8/site-packages (from requests->bentoml) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/frauke/anaconda3/lib/python3.8/site-packages (from requests->bentoml) (2021.10.8)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/frauke/anaconda3/lib/python3.8/site-packages (from schema->bentoml) (0.6.0.post1)\n",
      "Collecting h11>=0.8\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /home/frauke/anaconda3/lib/python3.8/site-packages (from anyio<4,>=3.0.0->watchfiles>=0.15.0->bentoml) (1.2.0)\n",
      "Collecting pep517>=0.9.1\n",
      "  Downloading pep517-0.13.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/frauke/.local/lib/python3.8/site-packages (from build->pip-tools>=6.6.2->bentoml) (2.0.1)\n",
      "Building wheels for collected packages: python-multipart\n",
      "  Building wheel for python-multipart (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=39ea1a4a102528167c3d19d873a238315f114d27e25156a9bf495cc2d2f9ddbf\n",
      "  Stored in directory: /home/frauke/.cache/pip/wheels/9e/fc/1c/cf980e6413d3ee8e70cd8f39e2366b0f487e3e221aeb452eb0\n",
      "Successfully built python-multipart\n",
      "Installing collected packages: deepmerge, commonmark, simple-di, rich, python-multipart, python-dotenv, pynvml, pep517, pathspec, opentelemetry-util-http, opentelemetry-semantic-conventions, h11, fs, exceptiongroup, deprecated, circus, asgiref, watchfiles, uvicorn, starlette, opentelemetry-api, cattrs, build, pip-tools, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-aiohttp-client, bentoml\n",
      "Successfully installed asgiref-3.5.2 bentoml-1.0.7 build-0.8.0 cattrs-22.2.0 circus-0.17.1 commonmark-0.9.1 deepmerge-1.0.1 deprecated-1.2.13 exceptiongroup-1.0.0rc9 fs-2.4.16 h11-0.14.0 opentelemetry-api-1.12.0 opentelemetry-instrumentation-0.33b0 opentelemetry-instrumentation-aiohttp-client-0.33b0 opentelemetry-instrumentation-asgi-0.33b0 opentelemetry-sdk-1.12.0 opentelemetry-semantic-conventions-0.33b0 opentelemetry-util-http-0.33b0 pathspec-0.10.1 pep517-0.13.0 pip-tools-6.9.0 pynvml-11.4.1 python-dotenv-0.21.0 python-multipart-0.0.5 rich-12.6.0 simple-di-0.1.5 starlette-0.21.0 uvicorn-0.18.3 watchfiles-0.17.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bentoml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b64a919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bentoml, version 1.0.7\r\n"
     ]
    }
   ],
   "source": [
    "!bentoml --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d98924",
   "metadata": {},
   "source": [
    "```import bentoml\n",
    "bentoml.xgboost.save_model(\"credit_risk_model\", model,\n",
    "                          custom_objects={\n",
    "                              \"dictVectorizer\": dv\n",
    "                          })```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3dafa8",
   "metadata": {},
   "source": [
    "* ```custom_objects``` allows to save other things we need for our model, as e.g. in this case the dictionary vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420a77d",
   "metadata": {},
   "source": [
    "Output:\n",
    "```Model(tag=\"credit_risk_model:4kf5u7coewdndaoi\", path=\"/home/frauke/bentoml/models/credit_risk_model/4kf5u7coewdndaoi/\")```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7520152",
   "metadata": {},
   "source": [
    "* This creates a unique tag each time 'save_model' is called\n",
    "* The model is saved at aspecific path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacfe934",
   "metadata": {},
   "source": [
    "### Create a Service\n",
    "\n",
    "* saved as 'service.py'\n",
    "* Call the service from the terimal: ```bentoml serve service.py:svc```\n",
    "* We then have a service running at ```localhost:3000```\n",
    "* We can use ```bentoml serve service.py:svc --reload``` to automatically reload the service, when we change it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ddad8",
   "metadata": {},
   "source": [
    "# 7.3 Deploying your Prediction Service\n",
    "\n",
    "* bentoml provides a command line tool:\n",
    "\n",
    "```bentoml models list```: lists all models saved.\n",
    "\n",
    "Output:\n",
    "\n",
    "| Tag |                                Module |          Size |       Creation Time |\n",
    "|-----|---------------------------------------|---------------|---------------------|\n",
    "| credit_risk_model:4kf5u7coewdndaoi | bentoml.xgboost | 195.66 KiB | 2022-10-17 16:13:33 |\n",
    "| credit_risk_model:fxjqrmcoekdndaoi | bentoml.xgboost | 195.27 KiB | 2022-10-17 15:47:02 |\n",
    "    \n",
    "```bentoml models get credit_risk_model:fxjqrmcoekdndaoi```: gives detailed information about the model\n",
    "\n",
    "```\n",
    "name: credit_risk_model                                                                                                             \n",
    "version: fxjqrmcoekdndaoi                                                                                                           \n",
    "module: bentoml.xgboost                                                                                                             \n",
    "labels: {}                                                                                                                          \n",
    "options:                                                                                                                            \n",
    "  model_class: Booster                                                                                                              \n",
    "metadata: {}                                                                                                                        \n",
    "context:                                                                                                                            \n",
    "  framework_name: xgboost                                                                                                           \n",
    "  framework_versions:                                                                                                               \n",
    "    xgboost: 1.6.2                                                                                                                  \n",
    "  bentoml_version: 1.0.7                                                                                                            \n",
    "  python_version: 3.8.3                                                                                                             \n",
    "signatures:                                                                                                                         \n",
    "  predict:                                                                                                                          \n",
    "    batchable: false                                                                                                                \n",
    "api_version: v2                                                                                                                     \n",
    "creation_time: '2022-10-17T13:47:02.302464+00:00'  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451ed3b9",
   "metadata": {},
   "source": [
    "### How to build our bento\n",
    "\n",
    "* We need to create a 'bentofile.yaml'\n",
    "* See documentation for complete list of possible parameters: https://docs.bentoml.org/en/latest/concepts/bento.html\n",
    "* It not only specifies things about the model itself, but also about the environment\n",
    "* build the bento: ```bentoml build``` in terminal\n",
    "* Output: ```Successfully built Bento(tag=\"credit_risk_classifier:tcr675covor57p7e\")```\n",
    "* Going to ```~/bentoml/bentos/credit_risk_classifier/tcr675covor57p7e``` we can see the files stored in the bento\n",
    "![bento1.png](bento1.png)\n",
    "* dockerfile is automatically build (can be customized)\n",
    "* Standardized way of combining all things needed for an ML service at one place\n",
    "* If we containerize it, we hava a single image to deploy \n",
    "* T actually containerize it, go back to the folder, where service.py and bentofile.yaml are stores, then in the terminal: ```bentoml containerize credit_risk_classifier:tcr675covor57p7e```\n",
    "* Output: ```Successfully built docker image for \"credit_risk_classifier\" with tags \"credit_risk_classifier:tcr675covor57p7e\"\n",
    "To run your newly built Bento container, pass \"credit_risk_classifier:tcr675covor57p7e\" to \"docker run\". For example: \"docker run -it --rm -p 3000:3000 credit_risk_classifier:tcr675covor57p7e serve --production\".```\n",
    "* Run ```docker run -it --rm -p 3000:3000 credit_risk_classifier:tcr675covor57p7e``` to start docker. Then we can go to ```localhost:3000``` to see our service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21c61f",
   "metadata": {},
   "source": [
    "# 7.4 Sending, Receiving and Validation Data\n",
    "\n",
    "* The service as it is at the moment also gives a response, when the input data is not valid, in the sense that some entry is missing or has a wrong name. To avoid this we use the library ```pydantic```\n",
    "* A list of input and output dscripters can be found in the documentation: https://docs.bentoml.org/en/latest/reference/api_io_descriptors.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3bd932",
   "metadata": {},
   "source": [
    "# 7.5 High-Performance Serving\n",
    "\n",
    "* Test service with big amount of traffig\n",
    "* Tool to send traffic to service: ```Locust```\n",
    "    * ```pip install locust```\n",
    "    * Need to create a ```locustfile```\n",
    "        * Contains a Data Sample\n",
    "        * Inherit from ```HttpUser```\n",
    "        * In this class, we need to create a ```task```, we call it ```classify```\n",
    "        * define a random waiting time, to simulate inconsistency\n",
    "        * T start the locust process: ```locust -H http://localhost:3000```\n",
    "* Optimizations\n",
    "    * Use ```async``` to parallize the requests, else all requests are done after each other\n",
    "    * ```async``` allows Parallalize at endpoint-level \n",
    "    * Traditionally, web scaling is done by replicating the entire process\n",
    "    * It is much more efficient to send multiple inputs to a model, instead of sending inputs one at a time\n",
    "    * Combine inputs to batches: ```micro-batching```\n",
    "    * We hvae to enable this, when we store the model:\n",
    "    ```bentoml.xgboost.save_model(\"credit_risk_model\", model,\n",
    "                          custom_objects={\n",
    "                              \"dictVectorizer\": dv\n",
    "                          },\n",
    "                          signatures={ # model signatures for micro-batching\n",
    "                              \"predict\": {\n",
    "                                  \"batchable\": True,\n",
    "                                  \"batch_dim\": 0\n",
    "                              }\n",
    "                          })```\n",
    "    * ```signature``` defines which endpoints of the model are going to be batchable\n",
    "    * We then need to run ```bentoml serve --production``` in the terminal\n",
    "    * This tells that we want more than 1 process for our webservice\n",
    "    * For more details see documetaion: www.docs.bentoml.or/en/latest/guides/batching.html#architecture\n",
    "    * Especially take care about the parameters: ```max_batch_size``` and ```max_latency_ms```\n",
    "    * These paramters can be changed creating a ```bentoconfiguration.yam``` file:\n",
    "    ```runners:\n",
    "         batching:\n",
    "           max_batch_size: 100\n",
    "           max_latency_ms: 500```\n",
    "    * In this file we can e.g. alos specify, if we want to run on a GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ced97e2",
   "metadata": {},
   "source": [
    "# 7.6 Bento Production Deployment\n",
    "* Build a bento, create a docker container, deploy docker container to AWS (EWS)\n",
    "* Build the bento: ```bentoml build```\n",
    "* Containerize: ```bentoml containerize credit_risk_classifier:3xlt5ocpp2ledn4h```\n",
    "* AWS.com: login, create 'Elastic Container Registry'\n",
    "    * After our container is build (previous step), we have to push it to the Container Registry on AWS\n",
    "    * We can use ```aws``` command as cli command to interface with our cloud\n",
    "    * install aws cli\n",
    "    * In order that aws cli works, we need our \"Access\" and \"Secret Keys\". These can be find at the link \"Security Credentials\" -> Access Keys. (Note that for real production cases there are more secure ways, e.g. \"IAM User\")\n",
    "    * To specify the access and secret key in the terminal use ````aws configure```\n",
    "    * To check, if we are connected: ```aws s3 ls``` shows what buckets are available\n",
    "    * Tag our created docker image: ```docker tag credit_risk_classifier:3xlt5ocpp2ledn4h ... ``` to our new repo (on AWS)\n",
    "    * Push that image: ```docker push ...```\n",
    "    * 'Elastic Container Registry' is the place to store the image, to run it we use 'Elastic Container Service'\n",
    "    * There we first need a 'Cluster' (click on 'Clusters' on the left hand side, then 'create cluster')\n",
    "    * There are different types of clusters, that we can create. For this example ww use 'Networking only', which makes for the most sence for entry-level AWS account (No GPUs).\n",
    "    * Configure task and container definitions:\n",
    "        * Task definition name: credit-risk-classifier\n",
    "        * Task role: -\n",
    "        * Operating system family: Linux\n",
    "        * Task memory (GB): 0.5GB\n",
    "        * Task CPU (vCGU): 0.25 vCPU\n",
    "     * These numbers are small, but will stay in the free tier!\n",
    "     * Press 'Add Container' button: Go back to Container Registry and copy the URI of the image\n",
    "         * Container name: credid-risk-classifier-container\n",
    "         * Image: <Image URI>\n",
    "         * Soft limit: 256 (This allows the applications to burst above the soft limit for short amounts of time, in contrast to a hard limit, for a larger model 'hard limit' makes sense)\n",
    "         * Port mappings: 3000 (we need to expose this port)\n",
    "         * When done, click \"add\"\n",
    "     * Go back to 'Clusters' and click on our cluster 'credit-risk-classifier-cluster'\n",
    "        * Go to 'Tasks'\n",
    "        * Run 'Run new Task' and specify options\n",
    "            * Launch type: Fargate\n",
    "            * Operating system family: Linux\n",
    "            * Family: credit-risk-classifier-task\n",
    "            * Revision: 2\n",
    "            * Cluster VPC: select default\n",
    "            * Subnets: ...\n",
    "            * Security Group: Click 'Edit'\n",
    "                * Select 'Inbound rules for security group': Type: Cluster ... , Port: 3000\n",
    "                * 'Save'\n",
    "            * 'Create'\n",
    "    * Now our task is running!\n",
    "    * We can use the puplic IP: <IP: 3000>\n",
    "    * Try 'locust' to see how it scales\n",
    "    \n",
    "* Alternatives:\n",
    "    * ECS, SageMaker: can hose Notebooks, a bit more expensive, can use GPUs\n",
    "    * Google\n",
    "    * Azure\n",
    "* Share Bento\n",
    "    * In terminal ```bentoml export```\n",
    "    * pushes the bento to a local file or to a remote destiation as e.g. a s3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e4f25b",
   "metadata": {},
   "source": [
    "# 7.7 - (Optional) Advanced Example: Deploying Stable Diffusion Model\n",
    "\n",
    "* https://github.com/bentoml/stable-diffusion-bentoml\n",
    "* Clone the repo: ```git clone https://github.com/bentoml/stable-diffusion-bentoml.git``` \n",
    "* Move into the repo: ```cd stable-diffusion-bentoml```\n",
    "* Create virtuel env: ```python3 -m venv venv```\n",
    "* Activate the env: ```. venv/bin/activate```\n",
    "* Update pip ```pip install -U pip```\n",
    "* Install requirements ```pip install -r requirements.txt```\n",
    "\n",
    "**There are different ways to create a stable Difference Bento**\n",
    "* Download a pre-build bento\n",
    "* Build from Stable Diffusion Models -> We will use this approach\n",
    "\n",
    "Choose a Stable Diffusion model: fp16 (for GPU with less than 10GB VRAM). This is a little bit smaller than the fp32\n",
    "* move to directory: ```cd fp16/```  \n",
    "* pull down the model and put it into the 'models' directory: ```curl https://s3.us-west-2.amazonaws.com/bentoml.com/stable_diffusion_bentoml/sd_model_v1_4_fp16.tgz | tar zxf - -C models/``` (this takes a bit)\n",
    "* move into the 'models' directory\n",
    "* use ```tree``` to get an overview about the model\n",
    "\n",
    "take a look at the bento: ```vim service.py```\n",
    "* We are instantiating a custom runner:\n",
    " ```stable_diffusion_runner = bentoml.Runner(StableDiffusionRunnable, name='stable_diffusion_runner', max_batch_size=10)```\n",
    "\n",
    "* create a service: ```svc = bentoml.Service(\"stable_diffusion_fp16\", runners=[stable_diffusion_runner])```\n",
    "\n",
    "* the api gets as input a JSON and the output is an image\n",
    "* stable diffusion as a second api, that takes as input text and image and outputs a text, this is also in our service. The input is a bit more complicated as it is 'multipart' (text+image)\n",
    "\n",
    "* The custom runner is defined as ```StableDiffusionRunnable```\n",
    "    * inherits from the ```bentoml.Runnable``` class\n",
    "    * Need to define ```SUPPORTED_RESSOURCES```: where should the modl run, what type of hardware (GPU?), ```SUPPORTED_CPU_THREADING``` if True multiple threads are created for the runner\n",
    "    *```__init___```: initializes where the model is and the device it is running on (here: cuda - driver for nvidia gpus), make sure everything we need in here to make a prediction\n",
    "    * ```txt2img```: runnable method with some preprocessing defined\n",
    "    * ```img2img```: runnable method with some (more complicated) preprocessing defined\n",
    "    \n",
    "take a look at the bentofile:\n",
    "* include the files and packages we need\n",
    "* docker options: the ```cuda_version``` is here important, if we want to run on a nvidia GPU!\n",
    "\n",
    "the configuration.yaml controls attributes of the runner\n",
    "* In this case only the timeout is made a larger, because the model needs at least a few seconds to run\n",
    "\n",
    "* build the bento: ```bentoml build```\n",
    "* We can't serve it locally wthout a GPU (will take a long time)\n",
    "* do ```pip install bentoctl```\n",
    "* There are different services supported to deploy on. we will use AWS EC2\n",
    "* move to ```stable-diffusion-bentoml/bentoctl``` and open ```deployment_config.yaml```, here aws-ec is defined and a GPU instance ('g4dn.xlarge', this is one of the cheaper GPU instances)\n",
    "* install operator: ```bentoctl operator install aws-ec2```\n",
    "* generate all files we need based on deployment_config.yaml: ```bentoctl enerate -f deployment_config.yaml```: creates ```./main.tf``` and ```bentoctl.tfvars```\n",
    "* install terraform: https://learn.hashicorp.com/tutorials/terraform/install-cli\n",
    "* terraform allows us to interface with AWS and deploy our bento to EC2, this is done using ```main.tf``` and ```bentoctl.tfvars```\n",
    "* build the docker conatiner, that we will deploy to EC2 and push it to a repository in our AWS account: ```bentoctl build -b stable_diffusion_fp16:latest```\n",
    "* EC2 has a few special reqirements for deploying docker, for locations that don't need any special packaging (like ECS, kubernetes) ```bento containerize``` is sufficient\n",
    "* create EC2 machine, that hosts the model: ```bentoctl apply -f deployment_config.yaml```\n",
    "\n",
    "To create the resources specifed run this after the build command.\n",
    "```$ bentoctl apply```\n",
    "\n",
    "To cleanup all the resources created and delete the registry run\n",
    "```$ bentoctl destroy```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0acc341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
