{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a2ff77d",
   "metadata": {},
   "source": [
    "# Tensorflow Serving\n",
    "\n",
    "* Context: clothing prediction\n",
    "* tf serving is especially created for serving tf models\n",
    "* a library written in C++\n",
    "* is only for inference\n",
    "* gets the already preprocessed data (image)\n",
    "* we need to create a \"Gateway\" that does the preprocessing\n",
    "\n",
    "![workflow](Screenshot_01.png)\n",
    "\n",
    "* For the gateway we will use Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e336a943",
   "metadata": {},
   "source": [
    "## TensorFlow Serving\n",
    "\n",
    "* Convert our trained model to the format of tf serving (saved_model format)\n",
    "* Run TF-Serving locally using docker\n",
    "* Invoking the model from Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e026eb6",
   "metadata": {},
   "source": [
    "Use a model from week8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24698bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d613d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x7f892413b400>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =\"../week8\"\n",
    "model = keras.models.load_model(os.path.join(path, \"xception_v4_05_0.850.h5\"))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86cbbec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: clothing_model/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model, 'clothing_model') # model, folder name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5a86d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3,5M\r\n",
      "drwxr-xr-x 2 frauke frauke 4,0K Mai  4 14:13 variables\r\n",
      "-rw-rw-r-- 1 frauke frauke 3,5M Mai  4 14:13 saved_model.pb\r\n",
      "drwxr-xr-x 2 frauke frauke 4,0K Mai  4 14:13 assets\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lrh clothing_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0df14ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mclothing_model\u001b[00m\r\n",
      "├── \u001b[01;34massets\u001b[00m\r\n",
      "├── saved_model.pb\r\n",
      "└── \u001b[01;34mvariables\u001b[00m\r\n",
      "    ├── variables.data-00000-of-00001\r\n",
      "    └── variables.index\r\n",
      "\r\n",
      "2 directories, 3 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree clothing_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6397b172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clothing_model:\r\n",
      "total 3,5M\r\n",
      "drwxr-xr-x 2 frauke frauke 4,0K Mai  4 14:13 assets\r\n",
      "-rw-rw-r-- 1 frauke frauke 3,5M Mai  4 14:13 saved_model.pb\r\n",
      "drwxr-xr-x 2 frauke frauke 4,0K Mai  4 14:13 variables\r\n",
      "\r\n",
      "clothing_model/assets:\r\n",
      "total 0\r\n",
      "\r\n",
      "clothing_model/variables:\r\n",
      "total 83M\r\n",
      "-rw-rw-r-- 1 frauke frauke 83M Mai  4 14:13 variables.data-00000-of-00001\r\n",
      "-rw-rw-r-- 1 frauke frauke 15K Mai  4 14:13 variables.index\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lRh clothing_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dfaff8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input_13'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 299, 299, 3)\n",
      "        name: serving_default_input_13:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['dense_7'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 10)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "\n",
      "Defined Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_13: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_13')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_13: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_13')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_13: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_13')\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_13: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_13')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          input_13: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='input_13')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir clothing_model --all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf53dd5c",
   "metadata": {},
   "source": [
    "* We are interested in ```signature_def['serving_default']``` -> save names of input and output in ```model-description.txt```\n",
    "* Now use docker to run tf-serving locally using this model:\n",
    "    * We use the official image from tensorflow\n",
    "    * ```\n",
    "       docker run -it --rm \\ \n",
    "       -p 8500:8500 \\\n",
    "       -v \"$(pdw)/clothing_model:/models/clothing-model/1 \\\n",
    "       -e MODEL_NAME=\"clothing-model\" \\\n",
    "       tensorflow/serving:2.7.0\n",
    "       ```\n",
    "    * the image maps port 8500\n",
    "    * we are mounting a \"volume\" - the model folder\n",
    "    * the name of this folder has to be the same as the ```MODEL_NAME```\n",
    "    * ```tensorflow/serving:2.7.0``` is the image name\n",
    " * Now send something to this model: code in notebook ```tf-serving-connect.ipynb```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b954150f",
   "metadata": {},
   "source": [
    "## Creating a Preprocessing Service\n",
    "\n",
    "* Convert the notebook to a python script\n",
    "* wrap the script into a flask app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fb5d31",
   "metadata": {},
   "source": [
    "## Run everything locally with Docker-Compose\n",
    "\n",
    "* Prepare the images\n",
    "* Install docker-compose (to run two services on one machine)\n",
    "* Run the service\n",
    "* Test the service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aca50d",
   "metadata": {},
   "source": [
    "## Introduction to Kubernetes\n",
    "\n",
    "* The anatomy of a Kubernetes cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9bd6dd",
   "metadata": {},
   "source": [
    "## Deploy a simple service to Kubernetes\n",
    "\n",
    "* Install kunectl\n",
    "* Set up a local Kubernetes cluster with Kind\n",
    "* Create a Deployment\n",
    "* Create a service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a0cbeb",
   "metadata": {},
   "source": [
    "## Deploy to EKS\n",
    "\n",
    "* Create a EKS cluster on AWS\n",
    "* Publish the image to ECR\n",
    "* Configure kubectl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1be457",
   "metadata": {},
   "source": [
    "## Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39f5af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
